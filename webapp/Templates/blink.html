<!-- blink.html -->
<!DOCTYPE html>
<html>
<head>
  <title>New MediaPipe Blink Detector</title>
</head>
<body>
    <h2>Blink Count: <span id="blink-count">0</span></h2>
<video id="video" autoplay muted playsinline width="640" height="480"></video>
<script type="module">
  import {
    FaceLandmarker,
    FilesetResolver
  } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

  let faceLandmarker;
  let blinkCount = 0;
  let lastBlink = false;
  let frameCount = 0;

  const video = document.getElementById('video');

  const loadLandmarker = async () => {
    const filesetResolver = await FilesetResolver.forVisionTasks(
      "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
    );

    faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
      baseOptions: {
        modelAssetPath: "https://storage.googleapis.com/mediapipe-assets/face_landmarker.task",
      },
      outputFaceBlendshapes: false,
      runningMode: "VIDEO",
      numFaces: 1,
    });

    startCamera();
  };

  const startCamera = async () => {
    const stream = await navigator.mediaDevices.getUserMedia({ video: true });
    video.srcObject = stream;

    video.onloadedmetadata = () => {
      video.play();
      requestAnimationFrame(processVideo);
    };
  };

  function getEAR(landmarks, indices) {
    const vertical1 = Math.hypot(
      landmarks[indices[1]].x - landmarks[indices[5]].x,
      landmarks[indices[1]].y - landmarks[indices[5]].y
    );
    const vertical2 = Math.hypot(
      landmarks[indices[2]].x - landmarks[indices[4]].x,
      landmarks[indices[2]].y - landmarks[indices[4]].y
    );
    const horizontal = Math.hypot(
      landmarks[indices[0]].x - landmarks[indices[3]].x,
      landmarks[indices[0]].y - landmarks[indices[3]].y
    );
    return (vertical1 + vertical2) / (2.0 * horizontal);
  }

  const LEFT_EYE = [33, 160, 158, 133, 153, 144];
  const RIGHT_EYE = [362, 385, 387, 263, 373, 380];

  const processVideo = async () => {
  if (!faceLandmarker) return;

  const now = performance.now();
  const results = faceLandmarker.detectForVideo(video, now);

  if (results.faceLandmarks && results.faceLandmarks.length > 0) {
    const landmarks = results.faceLandmarks[0];
    const leftEAR = getEAR(landmarks, LEFT_EYE);
    const rightEAR = getEAR(landmarks, RIGHT_EYE);
    const avgEAR = (leftEAR + rightEAR) / 2;

    const EAR_THRESHOLD = 0.23;
    const isBlinking = avgEAR < EAR_THRESHOLD;

    frameCount++;
    if (isBlinking && !lastBlink) {
      blinkCount++;
      document.getElementById("blink-count").textContent = blinkCount;  // ✅ Update UI
    }
    lastBlink = isBlinking;

    if (frameCount >= 300) {
      const blinkRate = blinkCount * 4; // blinks per minute
      fetch('/blink/save_blink_rate/', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'X-CSRFToken': '{{ csrf_token }}',
        },
        body: JSON.stringify({ rate: blinkRate })
      });
      blinkCount = 0;
      frameCount = 0;
      document.getElementById("blink-count").textContent = blinkCount;  // ✅ Reset UI
    }
  }

  requestAnimationFrame(processVideo);
};

  loadLandmarker();
</script>
</body>
</html>
